{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "class Predictor():\n",
    "\n",
    "    def __init__(self):\n",
    "        self._load()\n",
    "        self.tokenizer=Tokenizer()\n",
    "\n",
    "\n",
    "    def _load(self):\n",
    "        '''\n",
    "        initalize the model with a predefined architecture\n",
    "        and weights\n",
    "        '''\n",
    "        try:\n",
    "            json_file = open('../model/weights/model.json', 'r')\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "            loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "            loaded_model.load_weights(\"../model/weights/model_weights.h5\")\n",
    "            self.model=loaded_model\n",
    "        except:\n",
    "            print('Loading backup weights')\n",
    "            self._load_backup()\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _load_backup(self):\n",
    "        ''' \n",
    "        These are backu weights! We might update them once everything is over.\n",
    "        But they do not perform to well\n",
    "        '''\n",
    "\n",
    "        #Create an instance of the model architecture\n",
    "        json_file = open('../model/weights/model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "                \n",
    "\n",
    "        #load specific weights from np array\n",
    "        embedded = np.load('weights/backup/embedded_weights.npy')\n",
    "        lstm_weights = np.load('weights/backup/LSTM_weights.npy', allow_pickle=True)\n",
    "        dense_weights = np.load('weights/backup/dense_weights.npy',allow_pickle=True)\n",
    "        out_weights = np.load('weights//backupout_weights.npy',allow_pickle=True)\n",
    "\n",
    "        #manually put the weights in their place\n",
    "        loaded_model.layers[0].set_weights(embedded)\n",
    "        loaded_model.layers[1].set_weights(lstm_weights)\n",
    "        loaded_model.layers[4].set_weights(dense_weights)\n",
    "        loaded_model.layers[6].set_weights(out_weights)\n",
    "        \n",
    "        self.model = loaded_model\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def update_weights(path_to_weights):\n",
    "        '''\n",
    "        Update the weights of an existing architecture\n",
    "        If we have trained the current architecture and got better\n",
    "        results, etc.\n",
    "        input: path to the weights respectively from the folder\n",
    "                of this instance\n",
    "        return True if succeded or false if it failed\n",
    "        '''\n",
    "        try:\n",
    "            self.model.load_weights(path_to_weights)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_model(path_to_model, path_to_weights):\n",
    "        '''\n",
    "        Update the whole model with new weights.\n",
    "        Can be used if we have an instance of the model running\n",
    "        and want to deploy a new architecture\n",
    "        input: path to the model.json and weights.h5 for the new arch\n",
    "                They need to be respectively to the model place\n",
    "        return True if secceded and False if failed\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            json_file = open(path_to_model, 'r')\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "            loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "            loaded_model.load_weights(path_to_weights)\n",
    "            self.model=loaded_model\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def analyze(self,to_test):\n",
    "        '''\n",
    "        Use the model to analyze a text string and try to \n",
    "        find if it is fake or not\n",
    "        input: text string\n",
    "        return: prediction of the model\n",
    "        '''\n",
    "        word_freq = keras.preprocessing.text.text_to_word_sequence(to_test)\n",
    "        print(word_freq)\n",
    "        \n",
    "        tokenized = self.tokenizer.texts_to_sequences(texts=word_freq)\n",
    "        print(tokenized)\n",
    "        \n",
    "\n",
    "        acc = self.model.predict(tokenized)\n",
    "        print(acc)\n",
    "        #return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'we', 'are', 'cool']\n",
      "[[], [], [], []]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 4 arrays: [array([], shape=(0, 1), dtype=float64), array([], shape=(0, 1), dtype=float64), array([], shape=(0, 1), dtype=float64), array([], shape=(0, 1), dtype=float64)]...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-406e68be4762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Today we are cool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-3625bd73788f>\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, to_test)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m#return acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 4 arrays: [array([], shape=(0, 1), dtype=float64), array([], shape=(0, 1), dtype=float64), array([], shape=(0, 1), dtype=float64), array([], shape=(0, 1), dtype=float64)]..."
     ]
    }
   ],
   "source": [
    "pre.analyze('Today we are cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
