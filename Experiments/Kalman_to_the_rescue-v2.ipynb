{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Kalman_to_the_rescue.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2cgSihcjBTf",
        "colab_type": "text"
      },
      "source": [
        "# Kalman to the rescue!\n",
        "## Version 2\n",
        "An attempt to change the notebook from [this tutorial](https://towardsdatascience.com/bert-to-the-rescue-17671379687f) to our binarized liar dataset. Here, the model used is not the BERT sequence classifier implemented in the tutorial, but BertForSequenceClassification from the huggingface/transformers library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7_FXneEfpdMM",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "!pip install transformers pytorch-nlp\n",
        "# Install Tensorflow 2.X and Keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EnVIV6Vt8f4d",
        "scrolled": true,
        "outputId": "04f0c641-a146-4a8e-ebd7-a3cc5d729eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rn\n",
        "import torch\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from torch import nn\n",
        "from torchnlp.datasets import imdb_dataset\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cUYrv06z8gaF",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  rn.seed(321)\n",
        "  np.random.seed(321)\n",
        "  torch.manual_seed(321)\n",
        "  torch.cuda.manual_seed(321)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgkbhHcB17GY"
      },
      "source": [
        "## Prepare the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffu52YP91RIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(url):\n",
        "\t'''\n",
        "\tSimple function, that ready in the data, cleans it\n",
        "\tand returns it already split and train and test\n",
        "\t'''\n",
        "\tcomplete_data = pd.read_csv(url)\n",
        "\tcomplete_data.dropna()\n",
        "\ttexts = complete_data['content'].to_numpy()\n",
        "\n",
        "\tlabels = complete_data['label'].to_numpy()\n",
        "\n",
        "\tprint('Data will be returned as: ')\n",
        "\tprint('x_train, x_test, y_train, y_test')\n",
        "\treturn train_test_split(texts,labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ur8i7boP6qtb",
        "outputId": "5316d7cf-5866-4491-d47e-d169f2480e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/frietz58/euvsvirus/master/datasets/cleaned_data/liar_data_b.csv'\n",
        "x_train, x_test, y_train, y_test = get_data(url)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data will be returned as: \n",
            "x_train, x_test, y_train, y_test\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9591,), (9591,), (3197,), (3197,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w1KNAGeiqz9",
        "colab_type": "text"
      },
      "source": [
        "Check balance between classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F7POtHuIOV-6",
        "outputId": "3ae644cb-0886-4d58-e181-cd07211fb1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train.shape, y_test.shape, np.mean(y_train), np.mean(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9591,), (3197,), 0.4434365551037431, 0.4385361276196434)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWlWFN-KhQKP",
        "colab_type": "text"
      },
      "source": [
        "BERT tokenization: BERT was trained using the WordPiece tokenization. It means that a word can be broken down into more than one sub-words. This kind of tokenization is beneficial when dealing with out of vocabulary words, and it may help better represent complicated words. The sub-words are constructed during the training time and depend on the corpus the model was trained on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ty24UrRjqIsb",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "26trq3gIrJeG",
        "outputId": "96bfe85c-d8e5-4bc2-dab1-fa9a18194b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.tokenize('Hi my name is Pia bitches')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'my', 'name', 'is', 'pia', 'bitch', '##es']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxWLlx4whyFA",
        "colab_type": "text"
      },
      "source": [
        "The cell below creates the tokenizer, tokenizes each review, adds the special [CLS] token, and then takes only the first 512 tokens for both train and test sets (512 is the maximum sequence size for BERT):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1k9rcOzQr5Zm",
        "outputId": "99fdba01-2f2d-4dfa-8332-bd47f2b31cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], x_train))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], x_test))\n",
        "\n",
        "len(train_tokens), len(test_tokens)                   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9591, 3197)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH__XxakiFBW",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to convert each token in each review to an id as present in the tokenizer vocabulary. If thereâ€™s a token that is not present in the vocabulary, the tokenizer will use the special [UNK] token and use its id. Then we pad all sequences to size 512.\n",
        "\n",
        "NOTE: post-padding might impede an LSTM classifier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Ca7KKnhuT5c",
        "outputId": "d226330a-1829-4f2d-fe98-fc731901a6e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "\n",
        "train_tokens_ids.shape, test_tokens_ids.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9591, 512), (3197, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U-xXMEqXOWTE",
        "colab": {}
      },
      "source": [
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2K4JQMFo1-_S"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wdzjl_WlwpKr",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9jyb-hJ0xAgG",
        "colab": {}
      },
      "source": [
        "baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression(max_iter=10000)).fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q9IzjAX_2VLf",
        "colab": {}
      },
      "source": [
        "baseline_predicted = baseline_model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QnsCRIaQ3GPQ",
        "outputId": "3503506c-bdef-465d-ef59-9d46ac2f4827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "print(classification_report(y_test, baseline_predicted))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.71      0.67      1766\n",
            "           1       0.59      0.51      0.55      1431\n",
            "\n",
            "    accuracy                           0.62      3197\n",
            "   macro avg       0.61      0.61      0.61      3197\n",
            "weighted avg       0.62      0.62      0.62      3197\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r_hEhebQ3YqI"
      },
      "source": [
        "# Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E234ByBa3Qtb",
        "colab": {}
      },
      "source": [
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens, masks=None):\n",
        "        output = self.bert(tokens, attention_mask=masks)\n",
        "        pooled_output = output[1]\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ED9SE1Ka8W9x",
        "outputId": "f3d68b92-9dd8-47d2-e5b9-3e82fd5e41ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P4\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sf9n8zouENRi",
        "outputId": "b25be29c-96c6-468f-9fe1-436f6a7dab60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# This is the model implemented in the tutorial\n",
        "# bert_clf = BertBinaryClassifier()\n",
        "\n",
        "# We want to use the model provided by huggingface/transformers\n",
        "from transformers import BertForSequenceClassification\n",
        "bert_clf = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "if device.type == 'cuda':\n",
        "  bert_clf = bert_clf.cuda()\n",
        "  print('Memory Usage:')\n",
        "  print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "  print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory Usage:\n",
            "Allocated: 0.4 GB\n",
            "Cached:    0.5 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL78HXqe8Lcv",
        "colab_type": "text"
      },
      "source": [
        "Test for 3 samples if the model works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LOQ-870M7VWy",
        "outputId": "8057af90-0a08-494b-896d-a128c5cf52cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = torch.tensor(train_tokens_ids[:3]).to(device)\n",
        "y, pooled = bert_clf.bert(x)\n",
        "\n",
        "x.shape, y.shape, pooled.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 512]), torch.Size([3, 512, 768]), torch.Size([3, 768]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8rGc4mLYX_f",
        "colab_type": "text"
      },
      "source": [
        "*   x is of size (3, 512) , we took only 3 reviews, 512 tokens each.\n",
        "*   y is of size (3, 512, 768) , this is the BERTs final layer output for each token. Each token in each review is represented using a vector of size 768.\n",
        "*   pooled is of size (3, 768) this is the output of our [CLS] token, the first token in our sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LCb_pK4X7hb9",
        "outputId": "f9d47072-0c65-4415-e868-e5ad58f9846e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "y = bert_clf(x)[0]\n",
        "y.cpu().detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.13442509, -0.22923341],\n",
              "       [-0.05997267, -0.25016168],\n",
              "       [ 0.03254256, -0.22025113]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gil6CIpZb8hO",
        "colab_type": "text"
      },
      "source": [
        "**Note** This could be one value per class, i.e. the logits (classification scores) for real news (0) and fake news (1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KzzsUZOUFcxp",
        "outputId": "cab0f8c8-ea12-4ccd-814e-9a14fa0ca7ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "if device.type == 'cuda':\n",
        "  y, x, pooled = None, None, None\n",
        "  torch.cuda.empty_cache()\n",
        "  print('Memory Usage:')\n",
        "  print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "  print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "else:\n",
        "  y, x, pooled = None, None, None"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory Usage:\n",
            "Allocated: 0.4 GB\n",
            "Cached:    0.5 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c9LPIYcn99r8"
      },
      "source": [
        "# Fine-tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZUkXhM1k_TAl",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "EPOCHS = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jGwV0yqg_o2u",
        "colab": {}
      },
      "source": [
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(y_train.reshape(-1, 1))\n",
        "train_y_tensor = torch.nn.functional.one_hot(train_y_tensor, num_classes=2).squeeze().float()\n",
        "\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(y_test.reshape(-1, 1))\n",
        "test_y_tensor = torch.nn.functional.one_hot(test_y_tensor, num_classes=-1).squeeze().float()\n",
        "\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Yl2JpCe9YAu",
        "colab": {}
      },
      "source": [
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JF_QD0naS8EQ",
        "colab": {}
      },
      "source": [
        "# This is only used with BertBinaryClassifier\n",
        "# param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n",
        "# optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b28PcoDh_cyd",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(bert_clf.parameters(), lr=3e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z6yIChYvBP5F",
        "outputId": "5368d5f1-8b32-47d7-d6a7-76f1a1bc3b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "if device.type == 'cuda':\n",
        "  torch.cuda.empty_cache()\n",
        "  print('Memory Usage:')\n",
        "  print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "  print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory Usage:\n",
            "Allocated: 0.4 GB\n",
            "Cached:    0.5 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mqh8tCl4AFjo",
        "outputId": "b37a883e-55ee-450a-9246-5485de76a49c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    # counter = 0\n",
        "    \n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "        logits = bert_clf(token_ids, masks)[0]\n",
        "        \n",
        "        loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        batch_loss = loss_func(logits, labels)\n",
        "        train_loss += batch_loss.item()\n",
        "        \n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "\n",
        "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, math.floor(len(x_train) / BATCH_SIZE), train_loss / (step_num + 1)))\n",
        "\n",
        "        # counter += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  2\n",
            "\r4795/4795 loss: 0.6951577605344076 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpyKqTl2i3xS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if device.type == 'cuda':\n",
        "  torch.cuda.empty_cache()\n",
        "  print('Memory Usage:')\n",
        "  print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "  print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GHFWhkRYHv5l",
        "colab": {}
      },
      "source": [
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "        logits = bert_clf(token_ids, masks)[0]\n",
        "        \n",
        "        loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        loss = loss_func(logits, labels)\n",
        "        numpy_logits = logits.cpu().detach().numpy()\n",
        "        \n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "        all_logits += list(numpy_logits[:, 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vg_sX9BjooL-",
        "colab": {}
      },
      "source": [
        "np.mean(bert_predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-DmIJqUnkVM8",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_test, bert_predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}